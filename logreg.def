Bootstrap: docker
From: ubuntu:22.04

%labels
    Author "Your Name"
    Version "1.0"
    Description "Apptainer container to benchmark Logistic Regression on synthetic classification data"

%environment
    # Add pip-installed binaries to PATH and Python packages to PYTHONPATH
    export PATH=/usr/local/bin:$PATH
    export PYTHONPATH=/usr/local/lib/python3.10/site-packages:$PYTHONPATH

%post
    set -eux

    # 1) Update system and install Python + pip
    apt-get update && apt-get install -y --no-install-recommends \
        python3 python3-pip python3-venv \
        build-essential ca-certificates \
        && rm -rf /var/lib/apt/lists/*

    # 2) Upgrade pip and install required Python packages
    python3 -m pip install --no-cache-dir --upgrade pip setuptools wheel
    python3 -m pip install --no-cache-dir numpy pandas scikit-learn

    # 3) Create benchmarks folder
    mkdir -p /opt/benchmarks
    chown root:root /opt/benchmarks

    # 4) Create the Logistic Regression benchmark script
    cat << 'EOF' > /opt/benchmarks/train_logreg_benchmark.py
#!/usr/bin/env python3
"""
Benchmark Logistic Regression on synthetic classification data.
Measures training time, inference time, accuracy, and throughput.
"""

import argparse
import time
import datetime
import socket
import os
import csv
import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# -----------------------
# Parse command-line arguments
# -----------------------
parser = argparse.ArgumentParser(description="Logistic Regression benchmark")
parser.add_argument("--n-samples", type=int, default=5000, help="Number of synthetic samples")
parser.add_argument("--n-features", type=int, default=20, help="Number of features")
parser.add_argument("--n-informative", type=int, default=15, help="Number of informative features")
parser.add_argument("--n-classes", type=int, default=3, help="Number of classes")
parser.add_argument("--results-file", type=str, default="/opt/benchmarks/results_logreg.csv", help="CSV file for benchmark results")
args = parser.parse_args()

# -----------------------
# Generate synthetic dataset
# -----------------------
X, y = make_classification(
    n_samples=args.n_samples,
    n_features=args.n_features,
    n_informative=args.n_informative,
    n_classes=args.n_classes,
    random_state=42
)

scaler = StandardScaler()
X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# -----------------------
# Train Logistic Regression
# -----------------------
print(f"Training Logistic Regression on {len(X_train)} samples...")
train_start = time.perf_counter()
model = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')
model.fit(X_train, y_train)
train_end = time.perf_counter()
train_time = train_end - train_start

# -----------------------
# Inference benchmark
# -----------------------
inference_start = time.perf_counter()
y_pred = model.predict(X_test)
inference_end = time.perf_counter()
inference_time = inference_end - inference_start

accuracy = accuracy_score(y_test, y_pred)
throughput = len(X_test) / inference_time if inference_time > 0 else float('inf')

# -----------------------
# Print and save results
# -----------------------
timestamp = datetime.datetime.utcnow().isoformat() + "Z"
hostname = socket.gethostname()

summary = {
    "timestamp_utc": timestamp,
    "hostname": hostname,
    "n_samples": args.n_samples,
    "n_features": args.n_features,
    "n_classes": args.n_classes,
    "train_time_s": train_time,
    "inference_time_s": inference_time,
    "accuracy": accuracy,
    "throughput_samples_per_s": throughput
}

print("\n=== BENCHMARK SUMMARY ===")
for k, v in summary.items():
    print(f"{k}: {v}")
print("=========================")

# Save results to CSV
results_file = args.results_file
os.makedirs(os.path.dirname(results_file), exist_ok=True)
header = list(summary.keys())
write_header = not os.path.exists(results_file)

with open(results_file, "a", newline="") as f:
    writer = csv.DictWriter(f, fieldnames=header)
    if write_header:
        writer.writeheader()
    writer.writerow(summary)

print(f"Results appended to {results_file}")
EOF

    # 5) Make the script executable
    chmod +x /opt/benchmarks/train_logreg_benchmark.py

%runscript
    echo "Welcome to Logistic Regression Benchmark Container!"
    echo "Usage: train_logreg [options]"
    if [ "$1" = "train_logreg" ]; then
        shift
        exec python3 /opt/benchmarks/train_logreg_benchmark.py "$@"
    else
        exec "$@"
    fi

#i need to build the sif file ...
